{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e97d5878-9c89-441f-b434-4bf04b52d8c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated tags: ['смысл', 'проблема', 'автоматизация', 'слово', 'статья', 'сталкиваться']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/polinanazarova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import nltk\n",
    "import pymorphy2\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "nltk.download('stopwords') #Насколько я поняла, устанавливается 1 раз \n",
    "russian_stopwords = stopwords.words(\"russian\")\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Удаление пунктуации\n",
    "    words = text.split()\n",
    "    words = [word for word in words if word not in russian_stopwords]\n",
    "    return ' '.join(words)\n",
    "\n",
    "\n",
    "def extract_keywords_tfidf(text, top_n=5):\n",
    "    vectorizer = TfidfVectorizer(max_features=top_n)\n",
    "    tfidf_matrix = vectorizer.fit_transform([text])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    scores = tfidf_matrix.toarray().flatten()\n",
    "    keywords = sorted(zip(feature_names, scores), key=lambda x: x[1], reverse=True)\n",
    "    return [keyword for keyword, score in keywords]\n",
    "\n",
    "\n",
    "def generate_tags(text, top_n=round(len(text.split())*0.15)):\n",
    "    processed_text = preprocess_text(text)\n",
    "    \n",
    "    tfidf_keywords = extract_keywords_tfidf(processed_text, top_n)\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    tags = []\n",
    "    for i in list(set(tfidf_keywords))[:top_n]:\n",
    "        tags.append(morph.parse(i)[0].normal_form)\n",
    "    tags = set(tags)\n",
    "    \n",
    "    return list(tags)\n",
    "\n",
    "\n",
    "# Пример использования\n",
    "text = \"Полагаю, что неявно с проблемой многие сталкиваются ежедневно, после написания или анализа статьи, поста, комментария, заметки, отчета и т.д. Вот и мне по роду деятельности приходилось сталкиваться с данной проблемой по многу раз в день. Поэтому, можно сказать, к идее автоматизации меня привела «лень», в хорошем смысле этого слова.\"\n",
    "\n",
    "tags = generate_tags(text)\n",
    "print(f\"Generated tags: {tags}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eb4c0e4e-28a5-404d-8ff8-16ac67c3ed1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'автоматизация', 'проблема', 'слово', 'смысл', 'сталкиваться', 'статья'}"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "tags = []\n",
    "for i in bad_tags:\n",
    "    tags.append(morph.parse(i)[0].normal_form)\n",
    "tags = set(tags)\n",
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1779a1a9-d94c-4aed-932c-dca260b872ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /Users/polinanazarova/anaconda3/envs/vseross2024/lib/python3.10/site-packages (from rake-nltk) (3.9.1)\n",
      "Requirement already satisfied: click in /Users/polinanazarova/anaconda3/envs/vseross2024/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/polinanazarova/anaconda3/envs/vseross2024/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/polinanazarova/anaconda3/envs/vseross2024/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/polinanazarova/anaconda3/envs/vseross2024/lib/python3.10/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.66.5)\n",
      "Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: rake-nltk\n",
      "Successfully installed rake-nltk-1.0.6\n"
     ]
    }
   ],
   "source": [
    "!pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "cf98e20a-396c-4c4d-858f-4d49802fb1e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Название: Сын\n",
      "Описание: \n",
      "Это единственный в стране музей о жизни и трагической судьбе простой русской женщины  — епистинии федоровны степановой, которая положила на алтарь родины самое дорогое, что у нее было,  — жизнь своих сыновей. Многодетная мать вырастила девять сыновей, и все они погибли на фронтах разных войн. Старшего александра забрала у матери гражданская война, федора убили японцы в 1939-м, остальные сыны погибли в великую отечественную.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/polinanazarova/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/polinanazarova/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from collections import Counter\n",
    "import pymorphy2\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "stop_words = set(stopwords.words('russian'))\n",
    "\n",
    "\n",
    "def preprocess_text_for_text_data(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub('\\n', ' ', text)\n",
    "    text = re.sub('\\t', ' ', text)\n",
    "    text = re.sub(r'[^A-Za-zА-яа-я0-9\\s]', ' ', text)\n",
    "\n",
    "    words = [word for word in word_tokenize(text) if word not in stop_words]\n",
    "    return ' '.join(words)\n",
    "\n",
    "def preprocess_sentences(text):\n",
    "    sentences = sent_tokenize(text.lower())\n",
    "    return sentences\n",
    "\n",
    "def extract_keywords(text, num_keywords=5):\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=num_keywords)\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform([text])\n",
    "    keywords = tfidf_vectorizer.get_feature_names_out()\n",
    "    return keywords\n",
    "\n",
    "def extract_key_sentences(text, num_sentences=1):\n",
    "    sentences = preprocess_sentences(text)\n",
    "    \n",
    "    # Построение графа для TextRank\n",
    "    def sentence_similarity(sent1, sent2):\n",
    "        words1 = set(word_tokenize(sent1))\n",
    "        words2 = set(word_tokenize(sent2))\n",
    "        common_words = words1.intersection(words2)\n",
    "        return len(common_words) / (np.log(len(words1)) + np.log(len(words2)))\n",
    "    \n",
    "    similarity_matrix = np.zeros((len(sentences), len(sentences)))\n",
    "    \n",
    "    for idx1 in range(len(sentences)):\n",
    "        for idx2 in range(len(sentences)):\n",
    "            if idx1 != idx2:\n",
    "                similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2])\n",
    "    \n",
    "    nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "    scores = nx.pagerank(nx_graph)\n",
    "    \n",
    "    ranked_sentences = sorted(((scores[i], s) for i, s in enumerate(sentences)), reverse=True)\n",
    "    \n",
    "    key_sentences = [ranked_sentences[i][1] for i in range(min(num_sentences, len(ranked_sentences)))]\n",
    "    return key_sentences\n",
    "\n",
    "def generate_text_data(text, num_keywords=5, num_sentences=1):\n",
    "    # Извлечение ключевых слов с использованием TF-IDF\n",
    "    dirty_keywords = extract_keywords(preprocess_text_for_text_data(text), num_keywords)\n",
    "\n",
    "    morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "    keywords = []\n",
    "    for i in list(set(dirty_keywords)):\n",
    "        nf_word = morph.parse(i)[0].normal_form\n",
    "        if nf_word not in keywords: keywords.append(nf_word)\n",
    "    \n",
    "    # Извлечение ключевых предложений с использованием TextRank\n",
    "    key_sentences = extract_key_sentences(text, num_sentences=num_sentences)\n",
    "    \n",
    "    # Комбинирование ключевых слов и предложений для формирования заголовка\n",
    "    combined_keywords = ' '.join(keywords).capitalize()\n",
    "    for i in range(len(key_sentences)):\n",
    "        key_sentences[i] = key_sentences[i].capitalize()\n",
    "\n",
    "    combined_sentences = ' '.join(key_sentences)\n",
    "    \n",
    "    return combined_keywords,combined_sentences\n",
    "\n",
    "# Пример текста\n",
    "text = \"Это единственный в стране музей о жизни и трагической судьбе простой русской женщины  — Епистинии Федоровны Степановой, которая положила на алтарь Родины самое дорогое, что у нее было,  — жизнь своих сыновей. Многодетная мать вырастила девять сыновей, и все они погибли на фронтах разных войн. Старшего Александра забрала у матери гражданская война, Федора убили японцы в 1939-м, остальные сыны погибли в Великую Отечественную.\"\n",
    "\n",
    "# Генерация заголовка\n",
    "title, about = generate_text_data(text, num_keywords=1, num_sentences=3)\n",
    "print(f\"Название: {title}\")\n",
    "print(f\"Описание: \\n{about}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "690bfdc5-2174-4df1-8380-df4a7ed242c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement string (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for string\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f01bd430-801d-4554-94d1-295292433bc0",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'string' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[105], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mstring\u001b[49m\u001b[38;5;241m.\u001b[39mpunctuation)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'string' is not defined"
     ]
    }
   ],
   "source": [
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "a115577f-c67b-41eb-a125-c03999b5b9dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text.isalnum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16e2bfd8-dbf6-4561-a6c8-d4f3ba29a530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
